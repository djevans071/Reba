{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core routines for modeling is found in the file `modeling.py`. This notebook is for sample testing and analytics only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from matplotlib import pyplot as plt\n",
    "from workflow.data import *\n",
    "from workflow.features import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "\n",
    "plt.tight_layout\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "rc('xtick', labelsize=15) \n",
    "rc('ytick', labelsize=15) \n",
    "figure(figsize(10,7))\n",
    "cmap= sns.color_palette('Set1')\n",
    "sns.set_palette(cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connect to SQL database\n",
    "\n",
    "username = 'psam071'\n",
    "host = 'localhost'\n",
    "dbname = 'citibike'\n",
    "\n",
    "db = create_engine('postgres://%s%s/%s' % (username,host,dbname))\n",
    "con = None\n",
    "\n",
    "con = psycopg2.connect(database = dbname, user = username, host = host)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying Data and transforming it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# query stations from 2015 that existed at the beginning of the data\n",
    "# collecting period\n",
    "query_stations2015 = \"\"\"\n",
    "    SELECT DISTINCT a.id, name, lat, long, neighborhood, borough\n",
    "    FROM features a\n",
    "    LEFT JOIN stations b ON a.id = b.id\n",
    "    LEFT JOIN neighborhoods c on a.id = c.id\n",
    "    WHERE a.date = '2015-03-01'\n",
    "        --AND tot_docks > 0\n",
    "        AND borough = 'Manhattan'\n",
    "    ORDER BY a.id;\n",
    "\"\"\"\n",
    "\n",
    "stations2015 = pd.read_sql_query(query_stations2015, con)\n",
    "stations2015 = stations2015.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# most unbalanced stations\n",
    "query_unbal_stations = \"\"\"\n",
    "    SELECT a.id, abs(a.bikes_in - a.bikes_out) as flux,\n",
    "                 abs(a.rbikes_in - a.rbikes_out) as rflux,\n",
    "                 a.bikes_in, a.bikes_out,                                 \n",
    "            c.name, neighborhood, borough, long, lat\n",
    "    FROM   (SELECT id, min(date) as date, sum(bikes_out) as bikes_out, \n",
    "                sum(bikes_in) as bikes_in,\n",
    "                sum(rbikes_in) as rbikes_in, sum(rbikes_out) as rbikes_out\n",
    "            FROM features\n",
    "            GROUP BY id) a\n",
    "    JOIN neighborhoods b ON a.id = b.id\n",
    "    JOIN stations c on b.id = c.id\n",
    "    WHERE date = '2015-03-01'\n",
    "    ORDER BY rflux DESC\n",
    "    LIMIT 100;\n",
    "\"\"\"\n",
    "\n",
    "# make query and filter stations that existed at the beginning of the data collection phase\n",
    "df_unbal_stations = pd.read_sql_query(query_unbal_stations, con)\n",
    "# df_unbal_stations = df_unbal_stations[df_unbal_stations.id.isin(stations2015.id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save list of top 100 unbalanced stations to pickle file for webapp\n",
    "df_unbal_stations.to_pickle('websitetools/stations.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfcol_into_sqllist(df, col):\n",
    "    # converts a column in a pandas dataframe into a string for sql queries\n",
    "    listy = list(df[col].unique())\n",
    "    listy = listy[0:10]\n",
    "    return \"(\" + str(listy)[1:-1] + \")\"\n",
    "\n",
    "string_of_unbal_stations = str(list(df_unbal_stations.id.unique()))[1:-1]\n",
    "list_of_unbal_stations = list(df_unbal_stations.id)\n",
    "df_unbal_stations.id.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look at the patterns for the rebalanced stations\n",
    "# make sure tot_docks > 0 (especially when calculating bikes available)\n",
    "\n",
    "ids_to_see = dfcol_into_sqllist(df_unbal_stations, 'id')\n",
    "# ids_to_see = '(' + str(['72'])[1:-1] + ')'\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT a.id, a.date, a.hour, bikes_out, bikes_in, dayofweek, month, is_weekday,\n",
    "            is_holiday, rbikes_out, rbikes_in, tot_docks, avail_bikes, avail_docks,\n",
    "            precip, temp, long, lat, neighborhood, borough\n",
    "    FROM features a\n",
    "    LEFT JOIN weather b ON a.date = b.date AND a.hour = b.hour\n",
    "    LEFT JOIN stations c ON a.id = c.id\n",
    "    LEFT JOIN neighborhoods d ON a.id = d.id\n",
    "    WHERE a.id in {}\n",
    "        AND tot_docks > 0\n",
    "        AND borough = 'Manhattan'\n",
    "    --WHERE tot_docks > 0\n",
    "    ORDER BY a.id, a.date, a.hour;\n",
    "\"\"\".format(ids_to_see)\n",
    "\n",
    "df = pd.read_sql_query(query, con)\n",
    "df.date = pd.to_datetime(df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make new features (percentages)`\n",
    "df = new_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into 2015 (train) and 2016 (test) data\n",
    "# \n",
    "data_cols = ['id', 'long', 'lat', 'hour', 'month', 'dayofweek',\n",
    "             'is_weekday', \n",
    "             'is_holiday', 'precip', 'temp', 'pct_avail_bikes', \n",
    "             'pct_avail_docks']#, 'pct_avail_docks']\n",
    "# df = make_categorical(df, ['id', 'hour', 'month', 'is_weekday', 'is_holiday'])\n",
    "hist_cols = ['mean_flux', 'yest_flux', 'last_week_flux']\n",
    "\n",
    "df2015 = df[(df.date.dt.year == 2015)]\n",
    "df2016 = df[(df.date.dt.year == 2016)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prepare pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPOT Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tpot regressor\n",
    "\n",
    "# from tpot import TPOTRegressor\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # data = df[data_cols + hist_cols].sort_index()\n",
    "# # target = df.pct_flux\n",
    "\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(data, target, \n",
    "# #                                                     train_size = 0.75, test_size = 0.25)\n",
    "\n",
    "# X_train = df2015[data_cols]\n",
    "# y_train = df2015.pct_flux\n",
    "\n",
    "# X_test = df2016[data_cols]\n",
    "# y_test = df2016.pct_flux\n",
    "\n",
    "\n",
    "# reg = TPOTRegressor(generations=2, population_size = 5, verbosity=2)\n",
    "# reg.fit(X_train, y_train)\n",
    "# pred = tpot.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = [{'min_samples_leaf': [12, 14, 16],\n",
    "          'min_samples_split': [6, 8, 10],\n",
    "          'max_features': [0.85,0.95,1.]}]\n",
    "\n",
    "\n",
    "X_train = df2015[data_cols]\n",
    "y_train = df2015.pct_flux#.apply(flux_conditions, 0.15)\n",
    "\n",
    "X_test = df2016[data_cols]\n",
    "y_test = df2016.pct_flux#.apply(flux_conditions, 0.15)\n",
    "\n",
    "# reg = GridSearchCV(RandomForestRegressor(), params, cv=2, scoring = 'neg_mean_squared_error')\n",
    "reg = RandomForestRegressor(min_samples_leaf=16, min_samples_split=6, max_features = 0.95,\n",
    "                            n_jobs=-1)\n",
    "reg.fit(X_train, y_train)\n",
    "pred = reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.feature_importances_.round(2)\n",
    "importances = list(reg.feature_importances_.round(2))\n",
    "\n",
    "features_dict = {}\n",
    "for importance, col in zip(importances, data_cols):\n",
    "    features_dict[col] = importance\n",
    "\n",
    "feature_imp = pd.Series(features_dict)\n",
    "values_to_plot = feature_imp.sort_values()\n",
    "values_to_plot.rename(index = {'id':'Station ID',\n",
    "                                 'hour': 'Hour',\n",
    "                                 'pct_avail_bikes': 'Available Bikes',\n",
    "                                 'dayofweek': 'Day of the Week',\n",
    "                                 'is_weekday': 'Is a Weekday',\n",
    "                                 'temp': 'Temperature',\n",
    "                                 'precip': 'Precipitation',\n",
    "                                'month': 'Month',\n",
    "                              'lat': 'Station Latitude',\n",
    "                              'long': 'Station Longitude',\n",
    "                              'pct_avail_docks': 'Available Docks',\n",
    "                              'is_holiday': 'Is a Holiday'}, inplace = True)\n",
    "values_to_plot.plot(kind = 'barh', figsize=(7,7))\n",
    "xlabel('Feature Importance', fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pred_test(pred, y_test):\n",
    "    tests = pd.DataFrame()\n",
    "    tests['pct_flux_test'] = y_test.reset_index().pct_flux\n",
    "    tests['pct_flux_pred'] = pred\n",
    "    return tests\n",
    "\n",
    "def plot_pred_test(test,X_test):\n",
    "    tests.iloc[50050:51000].plot(figsize = (13,5), alpha = 0.5)\n",
    "    X_test.reset_index().iloc[50050:51000].pct_avail_bikes.plot(alpha = 0.3)\n",
    "    \n",
    "   \n",
    "tests = merge_pred_test(pred, y_test)\n",
    "# tests.pct_flux_test = tests.pct_flux_test.apply(flux_conditions, 0.2)\n",
    "# tests.pct_flux_pred = tests.pct_flux_pred.apply(flux_conditions, 0.2)\n",
    "plot_pred_test(tests, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_test_pred(X, y, pred):\n",
    "    pred_series = pd.Series(pred)\n",
    "    X = X.reset_index()\n",
    "    X['pct_flux_test'] = y.reset_index().pct_flux\n",
    "    X['pct_flux_pred'] = pred_series\n",
    "    return X\n",
    "\n",
    "df_compare = merge_test_pred(X_test, y_test, pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols  = ['pct_flux_pred', 'pct_flux_test', 'hour']\n",
    "weekday = 1\n",
    "dock_id = 477\n",
    "grps = df_compare.groupby(['id','is_weekday', 'hour']).mean().reset_index()\n",
    "grps_std = df_compare.groupby(['id','is_weekday', 'hour']).std().reset_index()\n",
    "cond = (grps.is_weekday ==  weekday) & (grps.id == dock_id)\n",
    "# grps[cond][cols].set_index('hour').plot()\n",
    "\n",
    "hr_profile = grps[cond][cols].set_index('hour')\n",
    "hr_profile_errors = grps_std[cond][cols].set_index('hour')\n",
    "\n",
    "x = hr_profile.pct_flux_pred\n",
    "error = hr_profile_errors.pct_flux_pred\n",
    "\n",
    "ax = x.plot(linewidth = 3, label = '2016 Prediction')\n",
    "fill_between(list(x.index), list(x - error), list(x + error), alpha = 0.2)\n",
    "hr_profile.pct_flux_test.plot(label = '2016 Data', linewidth = 3)\n",
    "labels = ['12 AM', '5 AM', '10 AM', '3 PM', '8 PM']\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "\n",
    "xlabel('Time of Day', size = 20)\n",
    "legend(loc = 2, prop = {'size':15})\n",
    "ylim([-0.45,0.45])\n",
    "suptitle('2016 Bike Flow for Station {} (Weekday)'.format(477))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def scoring_metrics(predicted, labeled):\n",
    "    mse = mean_squared_error(predicted, labeled)\n",
    "    r2 = r2_score(predicted, labeled)\n",
    "    \n",
    "    print 'MSE: {}'.format(mse)\n",
    "    print 'R2: {}'.format(r2)\n",
    "    \n",
    "    \n",
    "scoring_metrics(hr_profile.pct_flux_pred, hr_profile.pct_flux_test)\n",
    "#     return mse, r2\n",
    "\n",
    "# mean_squared_error(y_test, pred)\n",
    "# r2_score(y_test, pred)\n",
    "# explained_variance_score(y_test, pred)\n",
    "# RegressorMixin.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Miscellaneous Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction/observation plot\n",
    "\n",
    "line = linspace(-1,1, num = 50)\n",
    "\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "pl = fig.add_subplot(111)\n",
    "pl.scatter(pred, y_test, alpha = 0.1)\n",
    "pl.plot(line, line, c = 'k', linestyle = '--')\n",
    "xlabel('Predicted',fontsize = 15)\n",
    "ylabel('Observed',fontsize = 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# queries the entire features table to calculate pct_flux\n",
    "ids_to_see = dfcol_into_sqllist(df_unbal_stations, 'id')\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT a.id, a.date, a.hour, bikes_out, bikes_in, dayofweek, month, is_weekday,\n",
    "            is_holiday, rebal_net_flux, tot_docks, avail_bikes, avail_docks,\n",
    "            precip, snow, temp, c.long, c.lat\n",
    "    FROM features a\n",
    "    LEFT JOIN weather b ON a.date = b.date AND a.hour = b.hour\n",
    "    LEFT JOIN stations c ON a.id = c.id\n",
    "    WHERE tot_docks > 0 AND a.id in {}\n",
    "    ORDER BY a.id, a.date, a.hour;\n",
    "\"\"\".format(ids_to_see)\n",
    "\n",
    "df = pd.read_sql_query(query, con)\n",
    "df.date = pd.to_datetime(df.date)\n",
    "df = new_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# histogram of pct_flux compared to normal distribution\n",
    "\n",
    "from scipy.stats import norm\n",
    "x_axis = np.arange(-1,1,0.001)\n",
    "\n",
    "df.pct_flux.plot(kind = 'hist', logy = True, bins=400, normed = True, alpha = 0.5)\n",
    "plot(x_axis, norm.pdf(x_axis, df.pct_flux.mean(), df.pct_flux.std()))\n",
    "xlim([-0.5,0.5])\n",
    "ylim([0.001, None])\n",
    "xlabel('Bike Flow', fontsize = 15)\n",
    "ylabel('Frequency', fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['hours12'] = pd.to_datetime(df.hour, format='%H').dt.strftime('%I %p')\n",
    "\n",
    "cols  = ['pct_flux', 'pct_avail_bikes', 'hour']\n",
    "weekday = 1\n",
    "dock_id = 477\n",
    "grps = df.groupby(['id','is_weekday', 'hour']).mean().reset_index()\n",
    "cond = (grps.is_weekday == weekday) & (grps.id == dock_id)\n",
    "plotter = grps[cond][cols].sort_values('hour').set_index('hour')\n",
    "\n",
    "# plot1 = plotter['pct_avail_bikes'].plot(c = 'steelblue', label = 'Available Bikes')\n",
    "# change ticklabels\n",
    "# labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "labels = ['12 AM', '5 AM', '10 AM', '3 PM', '8 PM']\n",
    "plot1.set_xticklabels(labels)\n",
    "\n",
    "plot2 = plotter['pct_flux'].plot(c = 'r', label = 'Flow')\n",
    "legend(loc = 4, prop = {'size':15})\n",
    "xlabel('Time of Day', fontsize = 15)\n",
    "ylim([-.45, 0.7])\n",
    "\n",
    "\n",
    "suptitle('Bikes Activity for Station {} (Weekday)'.format(dock_id))\n",
    "\n",
    "# grps.xs((dock_id,weekday), level=('id', 'is_weekday'))[cols].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flux_by_hour(df, cols, dock_id, day = 0, month = 1):\n",
    "    grp_cols = ['id','month','dayofweek', 'hour']\n",
    "    grps = df.groupby(grp_cols).mean().reset_index()\n",
    "    if month:\n",
    "        cond = (grps.dayofweek == day) & (grps.month == month) & (grps.id == dock_id)\n",
    "    else:\n",
    "        cond = (grps.dayofweek == day) & (grps.id == dock_id)\n",
    "    return grps[cond].set_index('hour')[cols]\n",
    "\n",
    "def plot_by_hour(df, cols, dock_id, day = 0, month = 1):\n",
    "    df_hour = flux_by_hour(df, cols, dock_id, day = day, month = month)\n",
    "    df_hour.plot()\n",
    "    \n",
    "    #plot formatting\n",
    "    labels = ['12 AM', '5 AM', '10 AM', '3 PM', '8 PM']\n",
    "    plot1.set_xticklabels(labels)\n",
    "\n",
    "    plt.xlabel('Time of Day', size = 15)\n",
    "    legend(loc = 4, prop = {'size':15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unbal_stations_list = df_unbal_stations[df_unbal_stations.id.isin(stations2015.id)].id\n",
    "for id in list(unbal_stations_list)[0:5]:\n",
    "    plot_by_hour(df,['pct_flux', 'pct_avail_bikes'], id, day=3)\n",
    "    suptitle('Bike Activity for station {}'.format(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flux_profile = plot_by_hour(df,['pct_flux'], 477, day = 1, month=3)\n",
    "# flux_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "aggregators = {'bikes_in': 'sum', 'bikes_out': 'sum', 'long': 'max', 'lat': 'max'}\n",
    "df_morn = df_unbal_stations_byhr[df_unbal_stations_byhr.hour.isin([7,8,9])].groupby('id').agg(aggregators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map plot of difference between 8 am and 6 pm bike activity\n",
    "\n",
    "aggregators = {'bikes_in': 'sum', 'bikes_out': 'sum'}\n",
    "morn_cond = df_unbal_stations_byhr.hour.isin([8])\n",
    "even_cond = df_unbal_stations_byhr.hour.isin([17])\n",
    "grp_cols = ['id', 'lat', 'long', 'name']\n",
    "df_morn = df_unbal_stations_byhr[morn_cond].groupby(grp_cols).agg(aggregators).reset_index()\n",
    "df_even = df_unbal_stations_byhr[even_cond].groupby(grp_cols).agg(aggregators).reset_index()\n",
    "\n",
    "fig = plt.figure(figsize = (15,15))\n",
    "pl1 = fig.add_subplot(111)\n",
    "pl1.scatter(df_morn.long, df_morn.lat,\n",
    "           s = df_morn.bikes_out/50, color = 'r', alpha=0.9,\n",
    "           label = 'bikes out at 8 am')\n",
    "pl1.scatter(df_even.long, df_even.lat,\n",
    "           s = df_even.bikes_out/50, color = 'g', alpha=0.6,\n",
    "           label = 'bikes out at 6 pm')\n",
    "\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# norm = Normalize(start_station['trip count'].min(), start_station['trip count'].max())\n",
    "# Get dark tileset from CartoBD (https://cartodb.com/basemaps)\n",
    "tileset = r'http://{s}.basemaps.cartocdn.com/light_all/{z}/{x}/{y}.png'\n",
    "\n",
    "station_map = folium.Map(location = [40.73, -73.985], width = 400, height = 700,\n",
    "                        tiles = tileset,\n",
    "                        attr = '&copy; <a href=\"http://www.openstreetmap.org/copyright\">OpenStreetMap</a> contributors, &copy; <a href=\"http://cartodb.com/attributions\">CartoDB</a>',\n",
    "                        zoom_start = 13)\n",
    "\n",
    "\n",
    "for index, row in df_morn.iterrows():\n",
    "    morn_color = 'red' #rgb2hex(cm.YlOrRd(norm(row['trip count'])))\n",
    "    folium.CircleMarker(\n",
    "        location = [row['lat'], row['long']],\n",
    "        popup = row['name'],\n",
    "        radius = sqrt(row['bikes_out'])/15,\n",
    "        color = None, fill_color = morn_color).add_to(station_map)\n",
    "    \n",
    "for index, row in df_even.iterrows():\n",
    "    green_color = 'green' #rgb2hex(cm.YlOrRd(norm(row['trip count'])))\n",
    "    folium.CircleMarker(\n",
    "        location = [row['lat'], row['long']],\n",
    "        popup = row['name'],\n",
    "        radius = sqrt(row['bikes_out'])/15,\n",
    "        color = None, fill_color = green_color).add_to(station_map)\n",
    "    \n",
    "station_map    \n",
    "# station_map.save('station_map.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map plot of flow activity vs. rebalancing activity\n",
    "\n",
    "fig = plt.figure(figsize = (10,15))\n",
    "pl1 = fig.add_subplot(111)\n",
    "pl1.scatter(df_unbal_stations.long, df_unbal_stations.lat,\n",
    "           s = df_unbal_stations.flux/25, color = 'k', alpha=0.9,\n",
    "           label = 'total bike flow')\n",
    "pl1.scatter(df_unbal_stations.long, df_unbal_stations.lat,\n",
    "           s = df_unbal_stations.rflux/25, color = 'y', alpha=0.6,\n",
    "           label = 'total rebalancing flow')\n",
    "\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the most active stations by bikes_out\n",
    "query_stations_out = \"\"\"\n",
    "    SELECT a.id, bikes_out, c.name, neighborhood, borough\n",
    "    FROM   (SELECT id, sum(bikes_out) as bikes_out\n",
    "            FROM features\n",
    "            GROUP BY id) a\n",
    "    JOIN neighborhoods b ON a.id = b.id\n",
    "    JOIN stations c on b.id = c.id\n",
    "    ORDER BY bikes_out DESC;\n",
    "\"\"\"\n",
    "\n",
    "df_stations_out = pd.read_sql_query(query_stations_out, con)\n",
    "df_stations_out.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the most active stations by bikes_in\n",
    "query_stations_in = \"\"\"\n",
    "    SELECT a.id, bikes_in, c.name, neighborhood, borough\n",
    "    FROM   (SELECT id, sum(bikes_in) as bikes_in\n",
    "            FROM features\n",
    "            GROUP BY id) a\n",
    "    JOIN neighborhoods b ON a.id = b.id\n",
    "    JOIN stations c ON b.id = c.id\n",
    "    ORDER BY bikes_in DESC;\n",
    "\"\"\"\n",
    "\n",
    "df_stations_in = pd.read_sql_query(query_stations_in, con)\n",
    "df_stations_in.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the most active neighborhoods by bikes_out\n",
    "query_hoods_out = \"\"\"\n",
    "        SELECT sum(a.bikes_out) as bikes_out, \n",
    "                b.neighborhood, b.borough\n",
    "        FROM features a\n",
    "        JOIN neighborhoods b on a.id = b.id\n",
    "        GROUP BY borough, neighborhood\n",
    "        ORDER BY bikes_out DESC;\n",
    "\"\"\"\n",
    "\n",
    "df_hoods_out = pd.read_sql_query(query_hoods_out, con)\n",
    "df_hoods_out.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the most active neighborhoods by bikes_in\n",
    "query_hoods_in = \"\"\"\n",
    "        SELECT sum(a.bikes_in) as bikes_in, \n",
    "                b.neighborhood, b.borough\n",
    "        FROM features a\n",
    "        JOIN neighborhoods b on a.id = b.id\n",
    "        GROUP BY borough, neighborhood\n",
    "        ORDER BY bikes_in DESC;\n",
    "\"\"\"\n",
    "\n",
    "df_hoods_in = pd.read_sql_query(query_hoods_in, con)\n",
    "df_hoods_in.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the most unbalanced neighborhoods\n",
    "query_hoods_in = \"\"\"\n",
    "        SELECT sum(a.bikes_in - a.bikes_out) as flux, \n",
    "                b.neighborhood, b.borough\n",
    "        FROM features a\n",
    "        JOIN neighborhoods b on a.id = b.id\n",
    "        GROUP BY borough, neighborhood\n",
    "        ORDER BY flux DESC;\n",
    "\"\"\"\n",
    "\n",
    "df_hoods_in = pd.read_sql_query(query_hoods_in, con)\n",
    "df_hoods_in.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
